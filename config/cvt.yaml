train:
  batch_size: 8
  learning_rate: 0.001
  epochs: 100
  save_checkpoint: True
  save_checkpoint_freq: 2
  save_best_after: 10
  use_scheduler: True
  scheduler:
    name: "cosine"
    num_warmup_steps: 0
model:
  rnn_type: lstm
  name: levit_256x256
  input_size: 256
  hidden_size: 256
  vit:
    vit_model: cvt
    s1_emb_dim: 128 # stage 1 - dimension
    s1_emb_kernel: 7 # stage 1 - conv kernel size
    s1_emb_stride: 4 # stage 1 - conv stride
    s1_proj_kernel: 3 # stage 1 - attention ds-conv kernel size
    s1_kv_proj_stride: 2 # stage 1 - attention key / value projection stride
    s1_heads: 4 # stage 1 - heads
    s1_depth: 4 # stage 1 - depth
    s1_mlp_mult: 8 # stage 1 - feedforward expansion factor
    s2_emb_dim: 256 # stage 2 - (same as above)
    s2_emb_kernel: 3
    s2_emb_stride: 2
    s2_proj_kernel: 3
    s2_kv_proj_stride: 2
    s2_heads: 6
    s2_depth: 6
    s2_mlp_mult: 8
    s3_emb_dim: 512 # stage 3 - (same as above)
    s3_emb_kernel: 3
    s3_emb_stride: 2
    s3_proj_kernel: 3
    s3_kv_proj_stride: 2
    s3_heads: 7
    s3_depth: 7
    s3_mlp_mult: 8
    stages: ["s1", "s2", "s3"]
    mlp_last: 512
    dropout: 0.
